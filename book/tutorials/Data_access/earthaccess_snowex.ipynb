{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `earthaccess` to Search for and Download SnowEx Data from the NSIDC DAAC Server\n",
    "    \n",
    "## Tutorial Overview\n",
    "\n",
    "This notebook demonstrates how to search for and download NSIDC DAAC server hosted SnowEx 2023 data using the `earthaccess` package. SnowEx mission data have not yet migrated to the cloud and continue to be hosted at the NSIDC DAAC server. \n",
    "\n",
    "As an example data collection, we use SnowEx23 Mar23 IOP Snow Depth Measurements, Version 1 (Snex23_MAR23_SD) over the Alaska field sites. The data are stored in csv format with a metadata-rich header. \n",
    "\n",
    "We use `earthaccess`, an open source package developed by Luis Lopez (NSIDC developer) and a community of contributors, to allow easy search of the NASA Common Metadata Repository (CMR) and download of NASA data collections.  It can be used for programmatic search and access for both _DAAC-hosted_ and _cloud-hosted_ data. It manages authenticating using Earthdata Login credentials. `earthaccess` can be used to find and access both DAAC-hosted and cloud-hosted data in just **three** lines of code.  See [https://github.com/nsidc/earthaccess](https://github.com/nsidc/earthaccess).\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this tutorial you will learn:  \n",
    "1. how to use `earthaccess` to search for SnowEx data using a spatial filter and explore the search results;  \n",
    "2. how to download data granules to your hub space or local machine.    \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "The workflow described in this tutorial forms the initial steps of a _Download Model_ workflow that could be run on your local machine or on a AWS cloud compute resource.  You will need:\n",
    "\n",
    "1. a NASA Earthdata Login.  If you need to register for an Earthdata Login see the [Getting an Earthdata Login](https://icesat-2-2023.hackweek.io/preliminary/checklist/earthdata.html#getting-an-earthdata-login) section of the ICESat-2 Hackweek 2023 Jupyter Book.\n",
    "\n",
    "## Highly Recommended Viewing\n",
    "\n",
    "[earthaccess NASA Tech Spotlight video recording](https://www.youtube.com/watch?v=EIr3j1_wDc0)\n",
    "\n",
    "Watch a coding demonstration and learn about the history of earthaccess and the community that supports it.\n",
    "                                                  \n",
    "## Credits\n",
    "\n",
    "This notebook is based on an [NSIDC Data Tutorial](https://github.com/nsidc/NSIDC-Data-Tutorials) originally created by Luis Lopez and Mikala Beig, NSIDC, modified by Andy Barrett, NSIDC, and updated by Jennifer Roebuck and Gail Reckase, NSIDC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic data access via earthaccess\n",
    "\n",
    "[earthacces](https://earthaccess.readthedocs.io/en/latest) is a software library that provides an easy to use Python wrapper in front of the CMR and NASA DAAC API's.  Strengths of earthdata include: \n",
    "1. cross-DAAC data access (it's not specific to NSIDC).  \n",
    "2. Access to DAAC server and cloud hosted data. \n",
    "3. Easy authentication handling for Earthdata Login and AWS access keys and token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For searching NASA data\n",
    "import earthaccess\n",
    "\n",
    "# For nice printing of Python objects\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Authenticate\n",
    "\n",
    "The first step is to get the correct authentication that will allow us to access NASA Earthdata.  This is all done through Earthdata Login.  For accessing data that has been migrated to NASA's Earthdata Cloud environment the `login` method also retrieves and handles the correct AWS credentials.\n",
    "\n",
    "Login requires your Earthdata Login username and password. The `login` method will automatically search for these credentials as environment variables or in a `.netrc` file, and if those aren't available it will prompt us to enter our username and password. We use a `.netrc` strategy. A `.netrc` file is a text file located in our home directory that contains login information for remote machines.  If we don't have a `.netrc` file, `login` can create one for us.\n",
    "\n",
    "```\n",
    "earthaccess.login(strategy='interactive', persist=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = earthaccess.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Search for Collections (Data sets)\n",
    "\n",
    "`earthaccess` leverages the Common Metadata Repository (CMR) API to search for collections and granules.  [Earthdata Search](https://search.earthdata.nasa.gov/search) also uses the CMR API.\n",
    "\n",
    "We can use the `search_datasets` method to search for SnowEx collections by setting `keyword='SnowEx'`.\n",
    "\n",
    "This will display the number of data collections (data sets) that meet this search criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = earthaccess.search_datasets(keyword = 'SnowEx', provider = 'NSIDC_ECS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `search_datasets` method returns a python list of `DataCollection` objects. We can view the metadata for each collection in long form by passing a `DataCollection` object to print or as a summary using the `summary` method. We can also use the `pprint` function to Pretty Print each object.\n",
    "\n",
    "We will do this for the first 10 results (objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for collection in Query[:10]:\n",
    "    pprint.pprint(collection.summary(), sort_dicts=True, indent=4)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each collection, `summary` returns a subset of fields from the collection metadata and the Unified Metadata Model (UMM):\n",
    "- `concept-id` is a unique id for the collection. It consists of an alphanumeric code and the provider-id specific to the DAAC (Distributed Active Archive Center). You can use the `concept_id` to search for data granules.\n",
    "- `short_name` is a quick way of referring to a collection (instead of using the full title). It can be found on the collection landing page underneath the collection title after 'DATA SET ID'. See the table below for a list of the shortnames for ICESat-2 collections.\n",
    "- `version` is the version of each collection.\n",
    "- `file-type` gives information about the file format of the collection granules.\n",
    "- `get-data` is a collection of URLs that can be used to access the data, collection landing pages and data tools. \n",
    "- `cloud-info` this is for cloud-hosted data and provides additional information about the location of the S3 bucket that holds the data and where to get temporary AWS S3 credentials to access the S3 buckets. `earthaccess` handles these credentials and the links to the S3 buckets, so in general you won't need to worry about this information. \n",
    "\n",
    "Notice that the `concept-id` contains information about the locatio of the data: `NSIDC_ECS` and `NSIDC_CPRD`. `NSIDC_ECS` refers to the NSIDC local server and `NSIDC_CPRD` refers to NSIDC's _Earthatcloud-hosted_ collections, which you will see for the ICESat-2 collections currently in the cloud. \n",
    "\n",
    "`concept-id's` are unique identifiers for collections.  Alternatively, you can specify a `short-name` (and `version` if there are multiple public versions of a data set).\n",
    "\n",
    "For SnowEx, `ShortNames` are generally how different products are referred to.\n",
    "\n",
    "| ShortName | Product Description |\n",
    "|:--------------|:---------------------|\n",
    "| SNEX21_TS_SP        | SnowEx21 Time Series Snow Pits |\n",
    "| SNEX23_SSA_SO       | SnowEx23 Laser Snow Microstructure Specific Surface Area Snow-off Data |\n",
    "| SNEX23_Lidar        | SnowEx23 Airborne Lidar-Derived 0.25M Snow Depth and Canopy Height|\n",
    "| SNEX23_Lidar_Raw    | SnowEx23 Airborne Lidar Scans Raw|\n",
    "| SNEX23_BCEF_TLS     | SnowEx23 Bonanza Creek Experimental Forest Terrestrial Lidar Scans |\n",
    "| SNEX23_BCEF_TLS_Raw | SnowEx23 Bonanza Creek Experimental Forest Terrestrial Lidar Scans Raw|\n",
    "| SNEX23_SWE          | SnowEx23 Snow Water Equivalent |\n",
    "| SNEX23_MAR23_SD     | SnowEx23 Mar23 IOP Community Snow Depth Measurements |\n",
    "| SNEX23_SSA          | SnowEx23 Laser Snow Microstructure Specific Surface Area Data |\n",
    "| SNEX23_UW_GPR       | SnowEx23 University of Wyoming Ground Penetrating Radar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow Search using a Spatial Filter\n",
    "\n",
    "Here we are going to use a bounding box for the Alaska study areas to find SnowEx collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = earthaccess.search_datasets( \n",
    "  keyword = 'SnowEx',\n",
    "     bounding_box = (-149.597,64.699,-147.49,70.085),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eleven datasets were found within this bounding box.<br> As we did above, we will make a list of the datasets within this bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for collection in Query[:11]:\n",
    "    pprint.pprint(collection.summary(), sort_dicts=True, indent=4)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:  Search for data granules (files)\n",
    "First we will search for the SnowEx 23 Mar23 IOP Snow Depth Measurement collection using its short name: Snex23_Mar23_SD.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    short_name='Snex23_MAR23_SD',\n",
    "    bounding_box=(-149.597,64.699,-147.49,70.085)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Download Data Locally\n",
    "For this section, we will download the granule above from the SnowEx23 Mar23 IOP Snow Depth Measurements collection locally. <br>\n",
    "\n",
    "We'll download the file into a separate folder named \"tmp\", which will be created for us, if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_files = earthaccess.download(\n",
    "    results,\n",
    "    local_path='/tmp',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
